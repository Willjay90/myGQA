{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import yaml\n",
    "from torch.utils.data import DataLoader\n",
    "from train_model.dataset_utils import prepare_test_data_set\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from dataset_utils import text_processing\n",
    "from train_model.helper import run_model, build_model\n",
    "from IPython.display import Image, display, clear_output\n",
    "\n",
    "# Get test images\n",
    "# wget http://images.cocodataset.org/zips/test2015.zip, unzip and update path to image directory\n",
    "im_dir = '../data/image/test/test2015'\n",
    "# Get the models\n",
    "# wget https://dl.fbaipublicfiles.com/pythia/pretrained_models/most_data_models.tar.gz and \n",
    "# move the best_model.pth from the uncompressed file to the folder best_model\n",
    "# Get features\n",
    "# mkdir data\n",
    "# cd data\n",
    "# wget https://dl.fbaipublicfiles.com/pythia/data/vqa2.0_glove.6B.300d.txt.npy\n",
    "# wget https://dl.fbaipublicfiles.com/pythia/data/vocabulary_vqa.txt\n",
    "# wget https://dl.fbaipublicfiles.com/pythia/data/answers_vqa.txt\n",
    "# wget https://dl.fbaipublicfiles.com/pythia/data/imdb.tar.gz\n",
    "# wget https://dl.fbaipublicfiles.com/pythia/data/rcnn_10_100.tar.gz\n",
    "# wget https://dl.fbaipublicfiles.com/pythia/data/large_vocabulary_vqa.txt\n",
    "# wget https://dl.fbaipublicfiles.com/pythia/data/large_vqa2.0_glove.6B.300d.txt.npy\n",
    "# tar -xf rcnn_10_100.tar.gz\n",
    "# tar -xf imdb.tar.gz\n",
    "\n",
    "# config_file = 'results/default/1234/config.yaml'\n",
    "# model_file = 'results/default/data.image_fast_reader.false_1234/best_model.pth'\n",
    "\n",
    "config_file = 'pretrained_models/detectron_100_resnet_most_data/1234/config.yaml'\n",
    "model_file = 'pretrained_models/detectron_100_resnet_most_data/1234/best_model.pth'\n",
    "\n",
    "def get_image():\n",
    "    im_files = [f for f in os.listdir(im_dir)]\n",
    "    im_file = random.choice(im_files)\n",
    "    im_path = os.path.join(im_dir, im_file)\n",
    "    print(im_path)\n",
    "    clear_output()\n",
    "    display(Image(filename=im_path))\n",
    "    return im_file\n",
    "\n",
    "def get_imdb(im_file, question_str):\n",
    "    imdb = []\n",
    "    imdb.append({'dataset_name': 'vqa', 'version': 1, 'has_answer': False, 'has_gt_layout': False})\n",
    "    iminfo = {}\n",
    "    iminfo['image_name'] = im_file.replace('.jpg', '')\n",
    "    iminfo['img_id'] = int(iminfo['image_name'].split('_')[-1])\n",
    "    iminfo['question_id'] = 0\n",
    "    iminfo['feature_path'] = iminfo['image_name'] + '.npy'\n",
    "    iminfo['question_str'] = question_str\n",
    "    iminfo['question_tokens'] = text_processing.tokenize(iminfo['question_str'])\n",
    "    imdb.append(iminfo)\n",
    "    return imdb\n",
    "\n",
    "def print_result(question_ids, soft_max_result, ans_dic):\n",
    "    predicted_answers = np.argmax(soft_max_result, axis=1)\n",
    "    for idx, pred_idx in enumerate(predicted_answers):\n",
    "        question_id = question_ids[idx]\n",
    "        pred_ans = ans_dic.idx2word(pred_idx)\n",
    "        print(pred_ans)\n",
    "\n",
    "def demo():\n",
    "\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = yaml.load(f)\n",
    "    print(config['data'])\n",
    "\n",
    "    data_set_test = prepare_test_data_set(**config['data'], **config['model'], verbose=True, test_mode=True)\n",
    "#     myModel = build_model(config, data_set_test)\n",
    "#     myModel.load_state_dict(torch.load(model_file)['state_dict'])\n",
    "\n",
    "#     print('VQA Demo')\n",
    "#     print('Say next to go to next image')\n",
    "#     print('Say stop to stop demo')\n",
    "#     im_file = get_image()\n",
    "#     while(True):\n",
    "#         print(\"What question would you like to ask?\")\n",
    "#         question_str = input()\n",
    "#         if question_str.lower() == 'next':\n",
    "#             im_file = get_image()\n",
    "#             continue\n",
    "#         if question_str.lower() == 'stop':\n",
    "#             print('Bye')\n",
    "#             break\n",
    "#         data_set_test.datasets[0].imdb = get_imdb(im_file, question_str)\n",
    "#         data_reader_test = DataLoader(data_set_test, shuffle=False, batch_size=1)\n",
    "#         ans_dic = data_set_test.answer_dict\n",
    "\n",
    "#         question_ids, soft_max_result = run_model(myModel, data_reader_test, ans_dic.UNK_idx)\n",
    "#         print_result(question_ids, soft_max_result, ans_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 512, 'data_root_dir': 'data', 'dataset': 'vqa_2.0', 'image_depth_first': False, 'image_fast_reader': False, 'image_feat_test': ['detectron_fix_100/fc6/vqa/test2015,resnet152/vqa/test2015'], 'image_feat_train': ['detectron_fix_100/fc6/vqa/train2014,resnet152/vqa/train2014', 'detectron_fix_100/fc6/vqa/val2014,resnet152/vqa/val2014', 'detectron_fix_100/fc6/genome,resnet152/genome', 'detectron_fix_100/fc6/vqa_mirror/train2014,resnet152/vqa_mirror/train2014', 'detectron_fix_100/fc6/vqa_mirror/val2014,resnet152/vqa_mirror/val2014', 'detectron_fix_100/fc6/vqa/train2014,resnet152/vqa/train2014'], 'image_feat_val': ['detectron_fix_100/fc6/vqa/val2014,resnet152/vqa/val2014'], 'image_max_loc': 100, 'imdb_file_test': ['imdb/imdb_test2015.npy'], 'imdb_file_train': ['imdb/imdb_train2014.npy', 'imdb/imdb_val2014.npy', 'imdb/imdb_genome.npy', 'imdb/imdb_mirror_train2014.npy', 'imdb/imdb_mirror_val2014.npy', 'imdb/imdb_vdtrain.npy'], 'imdb_file_val': ['imdb/imdb_minival2014.npy'], 'num_workers': 5, 'question_max_len': 14, 'vocab_answer_file': 'answers_vqa.txt', 'vocab_question_file': 'vocabulary_vqa.txt'}\n",
      "imdb does not contain ground-truth layout\n",
      "Loading model and config ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/resnet152/vqa/test2015/COCO_test2015_000000262144.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0ac004a6afed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-3eb9e008a600>\u001b[0m in \u001b[0;36mdemo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mdata_set_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_test_data_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;31m#     myModel = build_model(config, data_set_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m#     myModel.load_state_dict(torch.load(model_file)['state_dict'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/pythia-master/train_model/dataset_utils.py\u001b[0m in \u001b[0;36mprepare_test_data_set\u001b[0;34m(**data_config)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_test_data_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mdata_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_fast_reader'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprepare_data_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imdb_file_test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image_feat_test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/workspace/pythia-master/train_model/dataset_utils.py\u001b[0m in \u001b[0;36mprepare_data_set\u001b[0;34m(imdb_file_label, image_dir_label, **data_config)\u001b[0m\n\u001b[1;32m     61\u001b[0m                                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                                     \u001b[0mtest_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                                     image_max_loc=image_max_loc)\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/pythia-master/dataset_utils/dataSet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, imdb_file, image_feat_directories, verbose, **data_params)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 self.imdb[self.first_element_idx]['feature_path'])\n\u001b[1;32m    182\u001b[0m             \u001b[0mimage_feat_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_feat_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             self.image_feat_readers.append(get_image_feat_reader(\n\u001b[1;32m    185\u001b[0m                 feats.ndim, self.image_depth_first, feats, self.image_max_loc))\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/resnet152/vqa/test2015/COCO_test2015_000000262144.npy'"
     ]
    }
   ],
   "source": [
    "demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
